%%
%% This is file `sample-acmsmall-conf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall-conf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-conf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmcopyright}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[IR'23, Master CS]{}{Spring 2023}{Leiden, the Netherlands}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Final assignment: Cross-encoder re-rankers}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Siwen Tu}
\author{Shupei Li}
\affiliation{%
  \institution{LIACS, Leiden University}
  \country{the Netherlands}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{S. Tu and S. Li}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{cross-encoder, ensemble methods, information retrieval}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

\section{Task 1: Evaluating cross-encoders}
\begin{table}[!ht]
    \centering
    \caption{Effectiveness results of three fine-tuned cross-encoder models}
    \label{tab:results-task1}
    \begin{tabular}{lllll}
        \toprule
        \textbf{Model} & \textbf{nDCG@10} & \textbf{Recall@100} & \textbf{MAP@1000} & \textbf{\#Training Steps}\\
        \midrule
        MiniLM & \textbf{0.696} & \textbf{0.507} & 0.450 & 47569 / 156250\\
        TinyBERT & 0.691 & 0.506 & \textbf{0.458} & 76301 / 156250\\
        distilroberta & 0.611 & 0.463 & 0.392 & 9036 / 156250\\
        \bottomrule
    \end{tabular}
\end{table}

\section{Task 2: Select and apply five ensemble methods}
\begin{table}[!ht]
    \centering
    \caption{Effectiveness results of five ensemble methods}
    \label{tab:results-task2}
    \begin{tabular}{llll}
       \toprule
       \textbf{Model} &  \textbf{nDCG@10} & \textbf{Recall@100} & \textbf{MAP@1000}\\
       \midrule
       Mixed & 0.707 & 0.513 & 0.465\\
       Reciprocal Rank Fusion (RRF) & 0.699 & 0.507 & 0.462\\
       BayesFuse & 0.697 & 0.507 & 0.451\\
       PosFuse & \textbf{0.710} & \textbf{0.516} & \textbf{0.480}\\
       Weighted BordaFuse & 0.707 & 0.511 & 0.464\\
       \bottomrule
    \end{tabular}
\end{table}

\section{Task 3: Analyzing most effective ensemble method}
The PosFuse ensemble method in Task 2 outperforms all other methods. Therefore, we apply PosFuse to all combinations of two models out of three in this task and summarize experimental results in Table \ref{tab:results-task3}.
\begin{table}[!ht]
    \centering
    \caption{Effectiveness results of all combinations}
    \label{tab:results-task3}
    \begin{tabular}{llll}
       \toprule
       \textbf{Combination} & \textbf{nDCG@10} & \textbf{Recall@100} & \textbf{MAP@1000}\\
       \midrule
       MiniLM + TinyBERT & \textbf{0.723} & \textbf{0.518} & \textbf{0.483}\\
       MiniLM + distilroberta & 0.691 & 0.509 & 0.463\\
       TinyBERT + distilroberta & 0.694 & 0.507 & 0.468\\
       \bottomrule
    \end{tabular}
\end{table}

MiniLM + TinyBERT ensemble performs best on all metrics. MiniLM + distilroberta performs worst on nDCG@10 and MAP@1000, while TinyBERT + distilroberta performs worst on Recall@100. The result is consistent with our expectations. Because MiniLM and TinyBERT achieve significantly better results than distilroberta in Task 1, we think the performance of the ensemble including distilroberta will be affected by the inferiority of distilroberta. There is no big difference between the performance of MiniLM + distilroberta and that of TinyBERT + distilroberta. However, MiniLM + TinyBERT performs much better than other combinations, which may be caused by removing the distilroberta ranking file. It is worth mentioning that MiniLM + TinyBERT even achieves higher scores than the ensemble of three models in Task 2. This pattern indicates that more models do not mean better performance. A weak model may introduce more noise than information, which impairs the model's performance.

\section{Task 4: Modifying the evaluation metric in fine-tuning}
We modify the CERerankingEvaluator class to change the evaluation metric to nDCG@10. We adopt the implementation in the sklearn package to calculate nDCG@10. Note that the modified class can be easily used by changing the corresponding API in the fine-tuning notebook. No other modification is needed. Our code has been included in the submission.

\section{Task 5: Ensembling through score injection}
In this task, we investigate the effect of the score injection. The score injection is a strategy described in paper \cite{injection} that aims at improving the performance of BERT-based re-rankers by injecting BM25 scores. However, we inject the average scores of MiniLM, TinyBERT, and distilroberta, instead of BM25 scores. Besides, we use the microsoft/MiniLM-L12-H384-uncased model as the re-ranker. Our modified version of the fine-tuning notebook first performs inference with three models and sets the average score as the first sentence of the document. Then the pairs of queries and injected documents are inputted in the re-ranker. Note that we convert the type of the average score into the integer following the suggestion in \cite{injection}. Due to the GPU quota of Colab, we fine-tune the microsoft/MiniLM-L12-H384-uncased model with and without injection for one hour and run the evaluation notebook. The evaluation results are reported in Table \ref{tab:results-task5}. We also include results in \cite{injection} for comparison.
\begin{table}[!ht]
    \centering
    \caption{Effectiveness results of ensembling with and without score injection}
    \label{tab:results-task5}
    \begin{tabular}{llll}
       \toprule
       \textbf{Model} & \textbf{nDCG@10} & \textbf{Recall@100} & \textbf{MAP@1000}\\
       \midrule
       MiniLM-L12-H384-uncased without injection & 0.668 & \textbf{0.503} & 0.450\\
       MiniLM-L12-H384-uncased with injection & 0.664 & 0.495 & 0.435\\
       $\text{MiniLM}_{\text{CAT}}$\cite{injection} & 0.704 & - & 0.452\\
       $\text{MiniLM}_{\text{BM25CAT}}$\cite{injection} & \textbf{0.711} & - & \textbf{0.463}\\
       \bottomrule
    \end{tabular}
\end{table}

MiniLM with BM25 injection has the best performance during evaluation. The performance of MiniLM with the injection of three BERT-based models' predictions does not achieve the anticipated effect. We think one reason is that the training process is not sufficient. Table \ref{tab:results-task5} indicates that our version of MiniLM without injection is not at the expected level as that of $\text{MiniLM}_{\text{CAT}}$. Another reason may be the instability of the model performance. We notice that MiniLM with the average score injection is not very robust during the training. The improvement of the model's robustness is a possible future direction for developing score injection strategies.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}
%%
%% If your work has an appendix, this is the place to put it.
\end{document}
\endinput
%%
%% End of file `sample-acmsmall-conf.tex'.
