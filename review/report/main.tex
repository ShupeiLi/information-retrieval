%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% personal
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{xurl}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\graphicspath{{./graphics}}

% Use the following line for the initial blind version submitted for review:

% \usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Critical Review}

\begin{document}

\twocolumn[
\icmltitle{Assignment: Critical Review\\CRET: Cross-Modal Retrieval Transformer for Efficient Text-Video Retrieval}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Siwen Tu (s3631400)}{to}
\icmlauthor{Shupei Li (s3430863)}{to}
\end{icmlauthorlist}

\icmlaffiliation{to}{LIACS, Leiden University, Leiden, Netherlands}

\icmlcorrespondingauthor{Siwen Tu}{s3631400@vuw.leidenuniv.nl}
\icmlcorrespondingauthor{Shupei Li}{s3430863@vuw.leidenuniv.nl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\section{Motivation}
Information can be stored and delivered through various carriers. For example, visual information is usually shown by images and acoustic information is usually saved in audios. In machine learning area, a modality refers to the data collected via the same information channel. Because of the diversity of information carriers, searching for information involves interactions between different modalities in general. During the class, we mainly focus on classical models whose input and output are both text. They have achieved satisfactory performance on document retrieval, however, can't be easily generalized to multimodality information retrieval tasks.

To process information from different modalities, we need to design a model that can represent and fuse data efficiently. Here we only consider two modalities --- text and videos, due to the wide application of the text-to-video retrieval operation. The task is formulated as follows. Given queries with the textual content, we are supposed to find relevent videos in databases. There are three challenges to complete this task: data representation, modality infusion, and feature alignment. Ji et al. propose a novel method called CRET in \cite{cret}, which successfully tackles challenges and achieves the state-of-the-art performance on text-to-video retrieval task. Next, we will introduce the CRET method in detail.

\section{Summary}
The \textbf{Embedding-based} methods and the \textbf{Model-based} methods are two popular methods in tackling the text-to-video retrieval tasks. The EDB method utilize the embeddings extracted from the text and video separately. While the EDB method suffers from the loss of accuracy which results from the loss of correspondence details, it shows significant advantage in terms of efficiency. In contrast, the MDB method superior the EDB in retrieval accuracy but suffers from low efficiency since it iterates all the text-video pairs to evaluate the distance without extracting explicit embeddings.\\

The authors of this paper proposed a novel EDB method named Cross-modal retrieval transformer, which solves conflict between efficiency and accuracy. The main contribution of this novel method consists of two parts, one is Cross-modal Correspondence Modeling $(CCM)$  module and the other is Gaussian Estimation of Embedding Space $(GEES)$ loss. \\

The proposed CRET model encodes the video and text separately.  The text encoder applies the BERT as the base model to encode features into global and local features. Meanwhile, the video encoder consists of the spatial and temporal encoders which extract the spatial and temporal features separately. The spatial encoder encodes all the features into global and local features. Then the model feed the global features into the temporal encoder to get the temporal embeddings. On the other hand, the local feature embeddings are projected to the same dimension as the text embeddings.\\

The Cross-modal Correspondence Modeling plays an important role in smoothing the gap between the domains of video and text modalities. We align these features using the transformer in which the text and video encoders share the same weights. In detail, we first calculate the distance between the token features and the query center, and regard the distance as the weights of this feature. Afterwards, we concatenate and project the aligned features from the multi-head module, and calculate the similarity score of these aligned features from text and video modalities.\\

The Gaussian Estimation of Embedding Space samples the video frame embeddings densely without too much computing power. The traditional loss function NCE requires trade-off between the accuracy and computational burden. The author improved this loss function by first making an assumption about the frame distribution then simplify and approximate the NCE function under this assumption. This improvement enhances the optimization efficiency of the SGD algorithm during the training process.\\

In the training process, the model considers the GEES loss and CCM loss together with a balancing parameter. In the Inference process, the model considers the distance of global and local features together with an adjustable parameter.\\

During the experiments, the author fed four datasets to the model and applied muiltiple metrics such as recall R@5, R@10 and MdR. The results shows that the CERT model demonstrates better performance over other state-of-art models even if some of these models are pre-trained on large video datasets. The author also did the ablation study to validate the effects of CCM and GEES loss.\\

In the end of the paper, the author validate the assumption in which the GEES loss based on and analyzed the quality of model based on the experiments.
\section{Assessment}
This paper clearly illustrates the principle of CRET method. And its structure is consistent with requirements of the scientific paper. First, authors review the previous works and summarize their contributions. Then, they make necessary assumptions to simplify the problem and describe CRET method in detail. It is worth mentioning that the validity of assumptions is proved by empirical experiments. Authors compare CRET and benchmarks on four data sets. They also conduct sblation studies. The process and results of experiments are documented in the paper, which support the effectiveness of CRET.

We examine the reproducibility of the paper from three aspects: source code, the availability of data sets, and experimental settings. It is unfortunate that authors don't make their code publicly available. Besides, authors affiliate with a financial services company, which increases the difficulty of accessing the source code. In the experimental setup section, authors introduce four datasets (MSRVTT, LSMDC, MSVD, DiDeMo) and implementation details. We check the accessibility of data sets one by one. LSMDC and DiDeMo can be downloaded from links listed in references. Official websites of MSRVTT and MSVD are no longer available. However, they can be downloaded from the open source community alternatively. As for experimental settings, authors have reported main parameter settings in the paper. But it is hard to use exactly the same experimental settings as the paper as a result of the inaccessibility of the source code.

The main contribution of this paper is the design of CRET model that achieves the state-of-the-art performance on text-to-video retrieval. CRET successfully deals with three common challenges in text-to-video retrieval operations. Specifically, CRET utilizes CaiT-S/24 to encode the video data and BERT to encode the text data. And the CCM module in CRET solves the problem of modality fusion as well as feature alignment. The paper points out that CRET can be extended to more methods and scenarios \cite{cret}. From the perspective of application, we think the CRET method has broad prospect in video-streaming websites and search engines.

Finally, we will discuss strong and weak points of this paper. One major advantage of this paper is fully considering the algorithm efficiency, while maintaining the satisfactory model performance. In the model design stage, authors introduce the multivariate Gaussian distribution assumption to approximate the loss function, which largely reduce the computing costs. In the experiment stage, authors accelerate the model training process by adopting pre-trained models, setting hyperparameters carefully, and distributing the training on several servers. Compared to baseline models, the CRET method doesn't need the extra pre-training on other data sets, which enhances its competitiveness and practicability. Another strong point of the paper is that authors conduct extensive ablation studies and hypothesis tests, which proves the validity of each part of the model. However, this paper also has some shortcomings that can be improved. First, the inaccessibility of the code has a great impact on reproducibility. Second, authors don't report the standard variance of experimental results and don't clearly state the number of times of running each model. Therefore, it is hard to assess the stability of CRET based on tables provided in the paper.
\bibliography{main}
\bibliographystyle{icml2021}
\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
